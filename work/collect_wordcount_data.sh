#!/usr/bin/env bash
set -euo pipefail

cd /opt/hibench

PROFILE="${PROFILE:-huge}"
CSV="/opt/hibench/report/wc_train_all.csv"
LOG="/opt/hibench/report/wordcount/spark/bench.log"
CONF="/opt/hibench/conf/spark.conf"
MAX_RUNS="${MAX_RUNS:-128}"   # чтобы не «взорваться» количеством прогонов :)

# --- гарантируем master/home в conf
if ! grep -q '^hibench.spark.master' "$CONF"; then
  echo 'hibench.spark.master     spark://spark-master:7077' >> "$CONF"
fi
if ! grep -q '^hibench.spark.home' "$CONF"; then
  echo 'hibench.spark.home       /opt/spark' >> "$CONF"
fi

# --- сетка параметров (малый grid, даёт 128 комбинаций)
# фиксируем executor.cores=2, driver.cores=1
EXEC_CORES="2"
DRIVER_CORES="1"

arr_EXEC_MEM=(2g 3g)                 # 2 значения
arr_EXEC_INST=(2 3)                  # 2 значения
arr_DRIVER_MEM=(2g)                  # 1 (фиксируем, чтобы держать 128)
arr_MAX_IN_FLIGHT=(48m 96m)          # 2
arr_FILE_BUF=(32k 128k)              # 2
arr_SHUFFLE_COMP=(true false)        # 2
arr_BCAST_BLOCK=(4m 8m)              # 2
arr_MEM_FRACTION=(0.4 0.6)           # 2

# --- заголовок CSV (если пуст)
if [[ ! -s "$CSV" ]]; then
  echo "profile,executor_cores,executor_memory,executor_instances,driver_cores,driver_memory,memory_fraction,shuffle_compress,broadcast_block,maxSizeInFlight,shuffle_file_buffer,duration_s,throughput_Bps,input_bytes,exit_code" > "$CSV"
fi

write_conf() {
  local ecores="$1" emem="$2" einst="$3" dcores="$4" dmem="$5" mfrac="$6" shcomp="$7" bblk="$8" infl="$9" sbuf="${10}"

  cat > "$CONF" <<EOF
# --- AUTOGENERATED by collect_wordcount_data.sh ---
hibench.spark.master     spark://spark-master:7077
hibench.spark.home       /opt/spark

spark.executor.cores     ${ecores}
spark.executor.memory    ${emem}
spark.executor.instances ${einst}

spark.driver.cores       ${dcores}
spark.driver.memory      ${dmem}

spark.memory.fraction    ${mfrac}
spark.shuffle.compress   ${shcomp}
spark.broadcast.blockSize ${bblk}
spark.reducer.maxSizeInFlight ${infl}
spark.shuffle.file.buffer ${sbuf}

spark.serializer         org.apache.spark.serializer.KryoSerializer
spark.eventLog.enabled   true
spark.eventLog.dir       /opt/spark/history
EOF
}

prepare_input_once() {
  # масштаб
  sed -i "s/^hibench.scale.profile.*/hibench.scale.profile        ${PROFILE}/" conf/hibench.conf

  # генерим вход (один раз)
  if ! /opt/hadoop/bin/hdfs dfs -test -e /Wordcount/Input/_SUCCESS; then
    echo ">>> prepare Wordcount (${PROFILE})..."
    bin/workloads/micro/wordcount/prepare/prepare.sh
  fi
}

run_once() {
  echo ">>> run Wordcount..."
  bin/workloads/micro/wordcount/spark/run.sh
}

parse_metrics() {
  local dur tp inb
  dur=$(awk '/Job 0 finished/{for(i=1;i<=NF;i++) if ($i=="took"){print $(i+1); exit}}' "$LOG" || true)
  [[ -z "$dur" ]] && dur=$(awk '/finished in [0-9]+\.[0-9]+ s/{for(i=1;i<=NF;i++) if ($i=="in"){print $(i+1); exit}}' "$LOG" || true)
  dur="${dur:-0}"

  inb=$(/opt/hadoop/bin/hdfs dfs -du -s /Wordcount/Input | awk '{print $1}' || echo 0)
  if awk "BEGIN{exit !($dur>0)}"; then
    tp=$(python3 - <<PY
d=float("$dur")
b=int("$inb")
print(int(b/d) if d>0 else 0)
PY
)
  else
    tp=0
  fi
  echo "$dur" "$tp" "$inb"
}

# --- сформируем полный список комбинаций
TMP_LIST=$(mktemp)
for emem in "${arr_EXEC_MEM[@]}"; do
for einst in "${arr_EXEC_INST[@]}"; do
for dmem in "${arr_DRIVER_MEM[@]}"; do
for mfrac in "${arr_MEM_FRACTION[@]}"; do
for shcomp in "${arr_SHUFFLE_COMP[@]}"; do
for bblk in "${arr_BCAST_BLOCK[@]}"; do
for infl in "${arr_MAX_IN_FLIGHT[@]}"; do
for sbuf in "${arr_FILE_BUF[@]}"; do
  echo "${EXEC_CORES},${emem},${einst},${DRIVER_CORES},${dmem},${mfrac},${shcomp},${bblk},${infl},${sbuf}" >> "$TMP_LIST"
done; done; done; done; done; done; done; done
# это 2*2*1*2*2*2*2*2 = 128

# --- при желании можно дополнительно рандомно отобрать подмножество
COMBOS=$(mktemp)
if command -v shuf >/dev/null 2>&1; then
  shuf -n "$MAX_RUNS" "$TMP_LIST" > "$COMBOS"
else
  head -n "$MAX_RUNS" "$TMP_LIST" > "$COMBOS"
fi
rm -f "$TMP_LIST"

# --- подготовим вход
prepare_input_once

# --- основной цикл
run_id=0
total=$(wc -l < "$COMBOS")
while IFS=',' read -r ecores emem einst dcores dmem mfrac shcomp bblk infl sbuf; do
  run_id=$((run_id+1))
  echo
  echo ">>> (${run_id}/${total}) params: emem=${emem} einst=${einst} mfrac=${mfrac} shcomp=${shcomp} bblk=${bblk} infl=${infl} sbuf=${sbuf}"

  write_conf "$ecores" "$emem" "$einst" "$dcores" "$dmem" "$mfrac" "$shcomp" "$bblk" "$infl" "$sbuf"

  # подчистить выход и гонять
  /opt/hadoop/bin/hdfs dfs -rm -r -skipTrash /Wordcount/Output >/dev/null 2>&1 || true
  set +e
  run_once
  rc=$?
  set -e

  read -r DUR TP INB < <(parse_metrics)

  echo "${PROFILE},${ecores},${emem},${einst},${dcores},${dmem},${mfrac},${shcomp},${bblk},${infl},${sbuf},${DUR},${TP},${INB},${rc}" >> "$CSV"

  # небольшой «breath»
  sleep 1
done < "$COMBOS"
rm -f "$COMBOS"

echo "=== DONE. CSV at $CSV ==="

#!/usr/bin/env bash
set -euo pipefail

cd /opt/hibench

PROFILE="${PROFILE:-huge}"
CSV="/opt/hibench/report/wc_train_all.csv"
LOG="/opt/hibench/report/wordcount/spark/bench.log"
CONF="/opt/hibench/conf/spark.conf"
MAX_RUNS="${MAX_RUNS:-64}"              # предел выборки из общей сетки
NUM_WORKERS="${NUM_WORKERS:-4}"

# --- гарантируем корректные master/home в спарк-конфиге
grep -q '^hibench.spark.master' "$CONF" || echo 'hibench.spark.master     spark://spark-master:7077' >> "$CONF"
grep -q '^hibench.spark.home'   "$CONF" || echo 'hibench.spark.home       /opt/spark'                >> "$CONF"

# --- базовые инварианты (сопоставимость и безопасность по ресурсам)
EXEC_CORES="2"
DRIVER_CORES="1"

# --- сетки параметров: компактнее для huge, шире для large
if [[ "$PROFILE" == "huge" ]]; then
  arr_EXEC_MEM=(2g 3g)                         # 2
  arr_EXEC_INST=(2 3)                          # 2
  arr_DRIVER_MEM=(2g)                          # 1
  arr_MEM_FRACTION=(0.4 0.6)                   # 2
  arr_SHUFFLE_COMP=(true false)                # 2
  arr_SPILL_COMP=(true false)                  # 2  <-- NEW
  arr_BCAST_BLOCK=(4m 8m)                      # 2
  arr_MAX_IN_FLIGHT=(48m 96m)                  # 2
  arr_FILE_BUF=(32k 128k)                      # 2
  arr_IO_CODEC=(lz4 snappy)                    # 2  <-- NEW
else
  # large — чуть шире диапазоны
  arr_EXEC_MEM=(2g 3g)                         # держим в безопасной зоне
  arr_EXEC_INST=(2 3)                          # экзекьюторов не больше воркеров
  arr_DRIVER_MEM=(2g)
  arr_MEM_FRACTION=(0.4 0.6 0.7)               # +0.7
  arr_SHUFFLE_COMP=(true false)
  arr_SPILL_COMP=(true false)
  arr_BCAST_BLOCK=(4m 8m)
  arr_MAX_IN_FLIGHT=(48m 64m 96m)              # +64m
  arr_FILE_BUF=(32k 64k 128k)                  # +64k
  arr_IO_CODEC=(lz4 snappy)
fi

# --- не позволяем executor.instances быть больше числа воркеров
_filtered_EXEC_INST=()
for ei in "${arr_EXEC_INST[@]}"; do
  [[ "$ei" -le "$NUM_WORKERS" ]] && _filtered_EXEC_INST+=("$ei")
done
arr_EXEC_INST=("${_filtered_EXEC_INST[@]}")

# --- заголовок CSV
if [[ ! -s "$CSV" ]]; then
  echo "profile,executor_cores,executor_memory,executor_instances,driver_cores,driver_memory,memory_fraction,shuffle_compress,spill_compress,broadcast_block,maxSizeInFlight,shuffle_file_buffer,io_codec,duration_s,throughput_Bps,input_bytes,exit_code" > "$CSV"
fi

write_conf() {
  local ecores="$1" emem="$2" einst="$3" dcores="$4" dmem="$5" mfrac="$6" shcomp="$7" spcomp="$8" bblk="$9" infl="${10}" sbuf="${11}" codec="${12}"

  cat > "$CONF" <<EOF
# --- AUTOGENERATED by collect_wordcount_data.sh ---
hibench.spark.master     spark://spark-master:7077
hibench.spark.home       /opt/spark

spark.executor.cores     ${ecores}
spark.executor.memory    ${emem}
spark.executor.instances ${einst}

spark.driver.cores       ${dcores}
spark.driver.memory      ${dmem}

spark.memory.fraction            ${mfrac}
spark.shuffle.compress           ${shcomp}
spark.shuffle.spill.compress     ${spcomp}
spark.broadcast.blockSize        ${bblk}
spark.reducer.maxSizeInFlight    ${infl}
spark.shuffle.file.buffer        ${sbuf}
spark.io.compression.codec       ${codec}

spark.serializer         org.apache.spark.serializer.KryoSerializer
spark.eventLog.enabled   true
spark.eventLog.dir       /opt/spark/history
EOF
}

prepare_input_once() {
  sed -i "s/^hibench.scale.profile.*/hibench.scale.profile        ${PROFILE}/" conf/hibench.conf
  if ! /opt/hadoop/bin/hdfs dfs -test -e /Wordcount/Input/_SUCCESS; then
    echo ">>> prepare Wordcount (${PROFILE})..."
    bin/workloads/micro/wordcount/prepare/prepare.sh
  fi
}

run_once() {
  echo ">>> run Wordcount..."
  bin/workloads/micro/wordcount/spark/run.sh
}

parse_metrics() {
  local dur tp inb
  dur=$(awk '/Job 0 finished/{for(i=1;i<=NF;i++) if ($i=="took"){print $(i+1); exit}}' "$LOG" || true)
  [[ -z "$dur" ]] && dur=$(awk '/finished in [0-9]+\.[0-9]+ s/{for(i=1;i<=NF;i++) if ($i=="in"){print $(i+1); exit}}' "$LOG" || true)
  dur="${dur:-0}"

  inb=$(/opt/hadoop/bin/hdfs dfs -du -s /Wordcount/Input | awk '{print $1}' || echo 0)
  if awk "BEGIN{exit !($dur>0)}"; then
    tp=$(python3 - <<PY
d=float("$dur"); b=int("$inb")
print(int(b/d) if d>0 else 0)
PY
)
  else
    tp=0
  fi
  echo "$dur" "$tp" "$inb"
}

# --- строим полный список комбинаций
TMP_LIST=$(mktemp)
for emem in "${arr_EXEC_MEM[@]}"; do
for einst in "${arr_EXEC_INST[@]}"; do
for dmem in "${arr_DRIVER_MEM[@]}"; do
for mfrac in "${arr_MEM_FRACTION[@]}"; do
for shcomp in "${arr_SHUFFLE_COMP[@]}"; do
for spcomp in "${arr_SPILL_COMP[@]}"; do
for bblk in "${arr_BCAST_BLOCK[@]}"; do
for infl in "${arr_MAX_IN_FLIGHT[@]}"; do
for sbuf in "${arr_FILE_BUF[@]}"; do
for codec in "${arr_IO_CODEC[@]}"; do
  echo "${EXEC_CORES},${emem},${einst},${DRIVER_CORES},${dmem},${mfrac},${shcomp},${spcomp},${bblk},${infl},${sbuf},${codec}" >> "$TMP_LIST"
done; done; done; done; done; done; done; done; done; done

# --- случайная подвыборка до MAX_RUNS
COMBOS=$(mktemp)
if command -v shuf >/dev/null 2>&1; then
  shuf -n "$MAX_RUNS" "$TMP_LIST" > "$COMBOS"
else
  head -n "$MAX_RUNS" "$TMP_LIST" > "$COMBOS"
fi
rm -f "$TMP_LIST"

# --- подготовка входных данных один раз
prepare_input_once

# --- основной цикл
run_id=0
total=$(wc -l < "$COMBOS")
while IFS=',' read -r ecores emem einst dcores dmem mfrac shcomp spcomp bblk infl sbuf codec; do
  run_id=$((run_id+1))
  echo
  echo ">>> (${run_id}/${total}) emem=${emem} einst=${einst} mfrac=${mfrac} shuf=${shcomp}/${spcomp} bblk=${bblk} infl=${infl} sbuf=${sbuf} codec=${codec}"

  write_conf "$ecores" "$emem" "$einst" "$dcores" "$dmem" "$mfrac" "$shcomp" "$spcomp" "$bblk" "$infl" "$sbuf" "$codec"

  /opt/hadoop/bin/hdfs dfs -rm -r -skipTrash /Wordcount/Output >/dev/null 2>&1 || true
  set +e
  run_once
  rc=$?
  set -e

  read -r DUR TP INB < <(parse_metrics)
  echo "${PROFILE},${ecores},${emem},${einst},${dcores},${dmem},${mfrac},${shcomp},${spcomp},${bblk},${infl},${sbuf},${codec},${DUR},${TP},${INB},${rc}" >> "$CSV"

  sleep 1
done < "$COMBOS"
rm -f "$COMBOS"

echo "=== DONE (${PROFILE}). CSV: $CSV ==="
